{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98df36bf",
   "metadata": {},
   "source": [
    "# S2F Performance Analysis\n",
    "\n",
    "Use this notebook to explore the prediction outputs produced by different S2F runs and compare their behaviour. Each run writes a `prediction.df` file inside `<installation_directory>/output/<alias>/`. Update the configuration cells below with the aliases you want to analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4652",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "\n",
    "The snippet below reads `s2f.conf` to locate the shared output directory. Adjust `CONFIG_PATH` if you are running the notebook from a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a30beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "CONFIG_PATH = Path('s2f.conf')\n",
    "config = configparser.ConfigParser()\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f'Configuration file not found: {CONFIG_PATH.resolve()}. Update CONFIG_PATH to match your setup.')\n",
    "config.read(CONFIG_PATH)\n",
    "\n",
    "BASE_OUTPUT = Path(config['directories']['installation_directory']).expanduser() / 'output'\n",
    "if not BASE_OUTPUT.exists():\n",
    "    raise FileNotFoundError(f'Output directory not found: {BASE_OUTPUT}. Make sure the S2F runs have been executed.')\n",
    "\n",
    "FILTERED_GOA_PATH = Path(config.get('databases', 'filtered_goa', fallback='filtered_goa')).expanduser()\n",
    "if not FILTERED_GOA_PATH.exists():\n",
    "    raise FileNotFoundError(f'Filtered GOA file not found: {FILTERED_GOA_PATH}. Update FILTERED_GOA_PATH below if your data lives elsewhere.')\n",
    "\n",
    "DEFAULT_OBO_PATH = Path('go.obo').resolve()\n",
    "if not DEFAULT_OBO_PATH.exists():\n",
    "    raise FileNotFoundError(f'GO ontology file not found: {DEFAULT_OBO_PATH}. Update DEFAULT_OBO_PATH to point at your go.obo file.')\n",
    "\n",
    "BASE_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde871d",
   "metadata": {},
   "source": [
    "## Discover available runs\n",
    "\n",
    "This cell lists the aliases that currently have diffusion outputs. If the list is long, slice or filter it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_RUNS = sorted(p.name for p in BASE_OUTPUT.iterdir() if p.is_dir())\n",
    "print(f\"{len(AVAILABLE_RUNS)} run(s) found under {BASE_OUTPUT}\")\n",
    "pd.DataFrame({'alias': AVAILABLE_RUNS})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5f3b6",
   "metadata": {},
   "source": [
    "## Select runs to compare\n",
    "\n",
    "Edit `RUNS_TO_COMPARE` to focus on the runs you are interested in. By default the cell keeps only aliases that actually exist in `AVAILABLE_RUNS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS_TO_COMPARE = [\n",
    "    'test_223283',\n",
    "    'test_223283_new',\n",
    "    # 'test_223283_new_2',\n",
    "]\n",
    "\n",
    "RUNS_TO_COMPARE = [alias for alias in RUNS_TO_COMPARE if alias in AVAILABLE_RUNS]\n",
    "if not RUNS_TO_COMPARE:\n",
    "    raise ValueError('Update RUNS_TO_COMPARE with at least one valid alias.')\n",
    "\n",
    "RUNS_TO_COMPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3f19",
   "metadata": {},
   "source": [
    "## Load prediction tables\n",
    "\n",
    "Helper functions to read the prediction scores and the index files written by each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253edffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Load the diffusion output for a single run as a tidy DataFrame.\"\"\"\n",
    "    path = base_path / alias / 'prediction.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Prediction file not found for {alias}: {path}')\n",
    "    df = pd.read_csv(path, sep='\t', header=None, names=['protein_id', 'term_id', 'score'])\n",
    "    df['alias'] = alias\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_terms(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Grab the GO term lookup table if you need term names or namespaces.\"\"\"\n",
    "    path = base_path / alias / 'terms.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'GO term index not found for {alias}: {path}')\n",
    "    return pd.read_pickle(path)\n",
    "\n",
    "\n",
    "def load_proteins(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Fetch the protein index for convenience (e.g. to map back to FASTA identifiers).\"\"\"\n",
    "    path = base_path / alias / 'proteins.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Protein index not found for {alias}: {path}')\n",
    "    return pd.read_pickle(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.concat(\n",
    "    [load_predictions(alias) for alias in RUNS_TO_COMPARE],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {predictions_df.shape[0]:,} scored annotations spanning {predictions_df['alias'].nunique()} run(s).\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036f5f2",
   "metadata": {},
   "source": [
    "## Summary statistics per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    predictions_df\n",
    "    .groupby('alias')\n",
    "    .agg(\n",
    "        proteins=('protein_id', 'nunique'),\n",
    "        go_terms=('term_id', 'nunique'),\n",
    "        annotations=('term_id', 'size'),\n",
    "        min_score=('score', 'min'),\n",
    "        median_score=('score', 'median'),\n",
    "        mean_score=('score', 'mean'),\n",
    "        max_score=('score', 'max')\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39bd17",
   "metadata": {},
   "source": [
    "## Score distribution (log scale)\n",
    "\n",
    "Scores tend to be very small, so plotting their log<sub>10</sub> values highlights differences between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['score_log10'] = np.log10(predictions_df['score'].clip(lower=1e-15))\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "sns.histplot(\n",
    "    data=predictions_df,\n",
    "    x='score_log10',\n",
    "    hue='alias',\n",
    "    bins=60,\n",
    "    element='step',\n",
    "    stat='density',\n",
    "    common_norm=False,\n",
    "    alpha=0.35\n",
    ")\n",
    "plt.xlabel('log10(score)')\n",
    "plt.ylabel('density')\n",
    "plt.title('Score distribution per run (log scale)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595328f",
   "metadata": {},
   "source": [
    "## Focus on top-N predictions per protein\n",
    "\n",
    "Restrict to the highest-scoring annotations per protein to study agreement between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac49a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 5  # change to inspect more or fewer annotations per protein\n",
    "\n",
    "_top_sorted = predictions_df.sort_values(\n",
    "    ['alias', 'protein_id', 'score'],\n",
    "    ascending=[True, True, False]\n",
    ")\n",
    "\n",
    "top_predictions = (\n",
    "    _top_sorted\n",
    "    .groupby(['alias', 'protein_id'], as_index=False)\n",
    "    .head(TOP_N)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138b928",
   "metadata": {},
   "source": [
    "## Pairwise comparison (first two runs)\n",
    "\n",
    "The cell below contrasts the first two aliases in `RUNS_TO_COMPARE`. Edit `BASELINE` and `VARIANT` if you prefer a different pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2:\n",
    "    BASELINE, VARIANT = RUNS_TO_COMPARE[0], RUNS_TO_COMPARE[1]\n",
    "\n",
    "    baseline_top = (\n",
    "        top_predictions[top_predictions['alias'] == BASELINE]\n",
    "        [['protein_id', 'term_id', 'score']]\n",
    "        .set_index(['protein_id', 'term_id'])\n",
    "    )\n",
    "    variant_top = (\n",
    "        top_predictions[top_predictions['alias'] == VARIANT]\n",
    "        [['protein_id', 'term_id', 'score']]\n",
    "        .set_index(['protein_id', 'term_id'])\n",
    "    )\n",
    "\n",
    "    pairwise = baseline_top.join(\n",
    "        variant_top,\n",
    "        how='outer',\n",
    "        lsuffix=f'_{BASELINE}',\n",
    "        rsuffix=f'_{VARIANT}'\n",
    "    )\n",
    "    pairwise[f'score_{BASELINE}'] = pairwise.get(f'score_{BASELINE}', 0).fillna(0)\n",
    "    pairwise[f'score_{VARIANT}'] = pairwise.get(f'score_{VARIANT}', 0).fillna(0)\n",
    "    pairwise['score_delta'] = pairwise[f'score_{VARIANT}'] - pairwise[f'score_{BASELINE}']\n",
    "    pairwise = pairwise.sort_values('score_delta', ascending=False)\n",
    "\n",
    "    print(f'Comparing top-{TOP_N} predictions between {BASELINE} and {VARIANT}.')\n",
    "    pairwise.head(20)\n",
    "else:\n",
    "    print('Add at least two aliases to RUNS_TO_COMPARE to compute pairwise deltas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa83f2",
   "metadata": {},
   "source": [
    "## Largest improvements and regressions\n",
    "\n",
    "Filter the `pairwise` table to highlight the strongest positive or negative score shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c52ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2 and 'pairwise' in globals():\n",
    "    gains = pairwise[pairwise['score_delta'] > 0].head(20)\n",
    "    losses = pairwise[pairwise['score_delta'] < 0].tail(20)\n",
    "\n",
    "    print('Top gains:')\n",
    "    display(gains)\n",
    "\n",
    "    print('Top losses:')\n",
    "    display(losses)\n",
    "else:\n",
    "    print('Pairwise deltas are not available — add at least two aliases and run the previous cell first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e49f9",
   "metadata": {},
   "source": [
    "## Per-protein overlap statistics\n",
    "\n",
    "Compute how much the top-N annotations overlap per protein between the first two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46094b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2:\n",
    "    BASELINE, VARIANT = RUNS_TO_COMPARE[0], RUNS_TO_COMPARE[1]\n",
    "    overlaps = []\n",
    "\n",
    "    for protein_id, subset in top_predictions.groupby('protein_id'):\n",
    "        alias_groups = {alias: group for alias, group in subset.groupby('alias')}\n",
    "        if BASELINE not in alias_groups or VARIANT not in alias_groups:\n",
    "            continue\n",
    "        baseline_terms = set(alias_groups[BASELINE]['term_id'])\n",
    "        variant_terms = set(alias_groups[VARIANT]['term_id'])\n",
    "        union = baseline_terms | variant_terms\n",
    "        if not union:\n",
    "            continue\n",
    "        shared = baseline_terms & variant_terms\n",
    "        overlaps.append({\n",
    "            'protein_id': protein_id,\n",
    "            'shared': len(shared),\n",
    "            'union': len(union),\n",
    "            'jaccard': len(shared) / len(union)\n",
    "        })\n",
    "\n",
    "    overlap_df = pd.DataFrame(overlaps)\n",
    "    print(f'Computed overlaps for {len(overlap_df)} proteins present in both runs.')\n",
    "    overlap_df.sort_values('jaccard', ascending=False).head(20)\n",
    "else:\n",
    "    print('Add at least two aliases to RUNS_TO_COMPARE to evaluate overlap.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f564c",
   "metadata": {},
   "source": [
    "## Optional: attach GO term metadata\n",
    "\n",
    "If you need GO term names or namespaces, use `load_terms` and merge on `term_id`. The snippet below shows how to expand the pairwise comparison with GO labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf042cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: enrich pairwise table with GO term metadata (adjust `ALIAS_FOR_METADATA` as needed)\n",
    "if RUNS_TO_COMPARE:\n",
    "    ALIAS_FOR_METADATA = RUNS_TO_COMPARE[0]\n",
    "    try:\n",
    "        terms_lookup = load_terms(ALIAS_FOR_METADATA)\n",
    "        terms_lookup = terms_lookup.reset_index().rename(columns={'term id': 'term_id'})\n",
    "    except FileNotFoundError as exc:\n",
    "        print(exc)\n",
    "    else:\n",
    "        if 'pairwise' in globals():\n",
    "            decorated = pairwise.reset_index().merge(terms_lookup, on='term_id', how='left')\n",
    "            decorated.head()\n",
    "        else:\n",
    "            print('Run the pairwise comparison cell first to create the `pairwise` table.')\n",
    "else:\n",
    "    print('RUNS_TO_COMPARE is empty; nothing to decorate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e16b2",
   "metadata": {},
   "source": [
    "## Ground truth configuration\n",
    "\n",
    "Use the helpers below to discover metadata for the available runs and set up GOA-based evaluation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _parse_taxon_from_fasta(fasta_path):\n",
    "    \"\"\"Infer a taxon identifier from the FASTA file name.\"\"\"\n",
    "    stem = Path(fasta_path).stem\n",
    "    match = re.search(r'(\\d+)$', stem)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    digits = re.findall(r'\\d+', stem)\n",
    "    return digits[-1] if digits else None\n",
    "\n",
    "\n",
    "def discover_run_metadata(run_config_paths):\n",
    "    \"\"\"Parse run configuration files to map aliases to useful metadata.\"\"\"\n",
    "    metadata = {}\n",
    "    for conf_path in run_config_paths:\n",
    "        parser = configparser.ConfigParser()\n",
    "        parser.read(conf_path)\n",
    "        if not parser.has_section('configuration'):\n",
    "            continue\n",
    "        alias = parser.get('configuration', 'alias', fallback='').strip()\n",
    "        if not alias:\n",
    "            continue\n",
    "        entry = metadata.setdefault(alias, {})\n",
    "        entry['run_config'] = conf_path\n",
    "        if parser.has_option('configuration', 'fasta'):\n",
    "            fasta_path = parser.get('configuration', 'fasta')\n",
    "            entry['fasta'] = fasta_path\n",
    "            taxon = _parse_taxon_from_fasta(fasta_path)\n",
    "            if taxon:\n",
    "                entry['taxon_id'] = taxon\n",
    "    return metadata\n",
    "\n",
    "\n",
    "RUN_CONFIG_PATHS = sorted(Path('.').glob('run*.conf'))\n",
    "RUN_METADATA = discover_run_metadata(RUN_CONFIG_PATHS)\n",
    "\n",
    "metadata_table = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'alias': alias,\n",
    "            'run_config': str(meta.get('run_config')),\n",
    "            'fasta': meta.get('fasta'),\n",
    "            'taxon_id': meta.get('taxon_id'),\n",
    "        }\n",
    "        for alias, meta in RUN_METADATA.items()\n",
    "    ]\n",
    ").sort_values('alias').reset_index(drop=True) if RUN_METADATA else pd.DataFrame(columns=['alias', 'run_config', 'fasta', 'taxon_id'])\n",
    "\n",
    "metadata_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a94b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EVIDENCE_CODES = [\n",
    "    code.strip()\n",
    "    for code in config.get('options', 'evidence_codes', fallback='EXP,IDA,IPI,IMP,IGI,IEP,TAS,IC').split(',')\n",
    "    if code.strip()\n",
    "]\n",
    "\n",
    "GROUND_TRUTH_CONFIG = {\n",
    "    alias: {\n",
    "        'taxon_id': meta.get('taxon_id'),\n",
    "        'goa_path': FILTERED_GOA_PATH,\n",
    "        'obo_path': DEFAULT_OBO_PATH,\n",
    "        'evidence_codes': DEFAULT_EVIDENCE_CODES,\n",
    "        'min_annotations_per_protein': 3,\n",
    "        'term_frequency_range': (1, 1_000_000),\n",
    "    }\n",
    "    for alias, meta in RUN_METADATA.items()\n",
    "    if meta.get('taxon_id')\n",
    "}\n",
    "\n",
    "# Override or add entries here as needed, for example:\n",
    "# GROUND_TRUTH_CONFIG['custom_alias'] = {\n",
    "#     'taxon_id': '123456',\n",
    "#     'goa_path': Path('/path/to/filtered_goa_subset.gaf'),\n",
    "#     'obo_path': Path('/path/to/go.obo'),\n",
    "#     'evidence_codes': ['EXP', 'IDA'],\n",
    "#     'min_annotations_per_protein': 3,\n",
    "#     'term_frequency_range': (1, 1_000_000),\n",
    "# }\n",
    "\n",
    "GROUND_TRUTH_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Measures.measures import HX_py\n",
    "from GOTool.GeneOntology import GeneOntology\n",
    "\n",
    "_GOA_CACHE = {}\n",
    "\n",
    "\n",
    "def load_goa_annotations_for_taxon(taxon_id, goa_path, evidence_codes, chunk_size=200_000):\n",
    "    \"\"\"Load GOA annotations for a single taxon from a potentially large GOA file.\"\"\"\n",
    "    goa_path = Path(goa_path)\n",
    "    if not goa_path.exists():\n",
    "        raise FileNotFoundError(f'GOA file not found: {goa_path}')\n",
    "    cache_key = (goa_path.resolve(), str(taxon_id), tuple(sorted(evidence_codes)) if evidence_codes else ())\n",
    "    if cache_key in _GOA_CACHE:\n",
    "        return _GOA_CACHE[cache_key].copy()\n",
    "\n",
    "    column_names = [\n",
    "        'DB', 'DB Object ID', 'DB Object Symbol', 'Qualifier', 'GO ID',\n",
    "        'DB Reference', 'Evidence Code', 'With', 'Aspect', 'DB Object Name',\n",
    "        'Synonym', 'DB Object Type', 'Taxon', 'Date', 'Assigned By',\n",
    "        'Annotation Extension', 'Gene Product Form ID'\n",
    "    ]\n",
    "    dtype = {name: str for name in column_names}\n",
    "    pattern = fr'(?:^|\\|)taxon:{taxon_id}(?:$|\\|)'\n",
    "    annotations = []\n",
    "\n",
    "    for chunk in pd.read_csv(\n",
    "        goa_path,\n",
    "        sep='\t',\n",
    "        comment='!',\n",
    "        header=None,\n",
    "        names=column_names,\n",
    "        dtype=dtype,\n",
    "        chunksize=chunk_size,\n",
    "        low_memory=False,\n",
    "    ):\n",
    "        mask = chunk['Taxon'].fillna('').str.contains(pattern, regex=True)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        subset = chunk.loc[mask]\n",
    "        if evidence_codes:\n",
    "            subset = subset[subset['Evidence Code'].isin(evidence_codes)]\n",
    "            if subset.empty:\n",
    "                continue\n",
    "        trimmed = subset[['DB Object ID', 'GO ID']].drop_duplicates()\n",
    "        trimmed = trimmed.rename(columns={'DB Object ID': 'Protein'})\n",
    "        trimmed['Score'] = 1.0\n",
    "        annotations.append(trimmed)\n",
    "\n",
    "    if annotations:\n",
    "        result = pd.concat(annotations, ignore_index=True)\n",
    "    else:\n",
    "        result = pd.DataFrame(columns=['Protein', 'GO ID', 'Score'])\n",
    "\n",
    "    _GOA_CACHE[cache_key] = result\n",
    "    return result.copy()\n",
    "\n",
    "\n",
    "def build_prediction_and_gold_matrices(predictions, annotations):\n",
    "    if predictions.empty:\n",
    "        raise ValueError('Prediction table is empty for the requested alias.')\n",
    "    if annotations.empty:\n",
    "        raise ValueError('Ground-truth annotations are empty for the requested taxon.')\n",
    "\n",
    "    proteins = sorted(set(predictions['protein_id']) | set(annotations['Protein']))\n",
    "    terms = sorted(set(predictions['term_id']) | set(annotations['GO ID']))\n",
    "\n",
    "    protein_to_idx = {protein: idx for idx, protein in enumerate(proteins)}\n",
    "    term_to_idx = {term: idx for idx, term in enumerate(terms)}\n",
    "\n",
    "    prediction_matrix = np.zeros((len(proteins), len(terms)), dtype=np.float32)\n",
    "    rows = predictions['protein_id'].map(protein_to_idx).to_numpy()\n",
    "    cols = predictions['term_id'].map(term_to_idx).to_numpy()\n",
    "    prediction_matrix[rows, cols] = predictions['score'].to_numpy(dtype=float)\n",
    "\n",
    "    gold_matrix = np.zeros((len(proteins), len(terms)), dtype=np.float32)\n",
    "    gt_rows = annotations['Protein'].map(protein_to_idx).to_numpy()\n",
    "    gt_cols = annotations['GO ID'].map(term_to_idx).to_numpy()\n",
    "    gold_matrix[gt_rows, gt_cols] = 1.0\n",
    "\n",
    "    return prediction_matrix, gold_matrix, protein_to_idx, term_to_idx\n",
    "\n",
    "\n",
    "def compute_information_content(term_to_idx, ontology, organism_name):\n",
    "    ic = np.zeros(len(term_to_idx), dtype=np.float32)\n",
    "    for term, idx in term_to_idx.items():\n",
    "        try:\n",
    "            ic[idx] = ontology.find_term(term).information_content(organism_name)\n",
    "        except KeyError:\n",
    "            ic[idx] = 0.0\n",
    "    return ic\n",
    "\n",
    "\n",
    "def filter_matrices(gold_matrix, prediction_matrix, ic_vector, min_annotations_per_protein=3, term_frequency_range=(1, 1_000_000)):\n",
    "    lower, upper = term_frequency_range\n",
    "    upper = float('inf') if upper is None else upper\n",
    "\n",
    "    sumrow = gold_matrix.sum(axis=1)\n",
    "    sumcol = gold_matrix.sum(axis=0)\n",
    "\n",
    "    row_mask = sumrow >= min_annotations_per_protein\n",
    "    col_mask = (sumcol >= lower) & (sumcol <= upper)\n",
    "\n",
    "    filtered_pred = prediction_matrix[row_mask][:, col_mask]\n",
    "    filtered_gold = gold_matrix[row_mask][:, col_mask]\n",
    "    filtered_ic = ic_vector[col_mask]\n",
    "\n",
    "    return filtered_pred, filtered_gold, filtered_ic, row_mask, col_mask\n",
    "\n",
    "\n",
    "def evaluate_alias(alias, predictions_df, alias_cfg):\n",
    "    alias_predictions = predictions_df[predictions_df['alias'] == alias].copy()\n",
    "    if alias_predictions.empty:\n",
    "        raise ValueError('No predictions found for this alias in predictions_df.')\n",
    "\n",
    "    taxon_id = alias_cfg.get('taxon_id')\n",
    "    if not taxon_id:\n",
    "        raise ValueError('Taxon identifier is missing from GROUND_TRUTH_CONFIG.')\n",
    "\n",
    "    goa_path = alias_cfg.get('goa_path', FILTERED_GOA_PATH)\n",
    "    evidence_codes = alias_cfg.get('evidence_codes', DEFAULT_EVIDENCE_CODES)\n",
    "    annotations = load_goa_annotations_for_taxon(taxon_id, goa_path, evidence_codes)\n",
    "    if annotations.empty:\n",
    "        raise ValueError('No annotations retrieved from GOA for the selected taxon and evidence codes.')\n",
    "\n",
    "    organism_name = f'gt_{alias}'\n",
    "    ontology_path = alias_cfg.get('obo_path', DEFAULT_OBO_PATH)\n",
    "    ontology = GeneOntology(str(ontology_path), verbose=False)\n",
    "    ontology.build_structure()\n",
    "    ontology.load_annotations(annotations, organism_name)\n",
    "    ontology.up_propagate_annotations(organism_name)\n",
    "    propagated_annotations = ontology.get_annotations(organism_name)\n",
    "\n",
    "    prediction_matrix, gold_matrix, protein_to_idx, term_to_idx = build_prediction_and_gold_matrices(alias_predictions, propagated_annotations)\n",
    "    ic_vector = compute_information_content(term_to_idx, ontology, organism_name)\n",
    "\n",
    "    freq_range = alias_cfg.get('term_frequency_range', (1, 1_000_000))\n",
    "    min_ann = alias_cfg.get('min_annotations_per_protein', 3)\n",
    "    filtered_pred, filtered_gold, filtered_ic, row_mask, col_mask = filter_matrices(\n",
    "        gold_matrix,\n",
    "        prediction_matrix,\n",
    "        ic_vector,\n",
    "        min_annotations_per_protein=min_ann,\n",
    "        term_frequency_range=freq_range,\n",
    "    )\n",
    "\n",
    "    if filtered_gold.size == 0 or filtered_gold.sum() == 0:\n",
    "        raise ValueError('No overlapping annotations left after filtering criteria were applied.')\n",
    "\n",
    "    if np.unique(filtered_pred).size > 10000:\n",
    "        filtered_pred = np.around(filtered_pred, decimals=4)\n",
    "\n",
    "    measure = HX_py(filtered_pred, filtered_ic, organism_id=alias, verbose=False)\n",
    "    results = {\n",
    "        'overall': measure.compute_overall(filtered_gold),\n",
    "        'per_gene': measure.compute_per_gene(filtered_gold),\n",
    "        'per_term': measure.compute_per_term(filtered_gold),\n",
    "    }\n",
    "    context = {\n",
    "        'proteins_considered': int(row_mask.sum()),\n",
    "        'terms_considered': int(col_mask.sum()),\n",
    "        'total_annotations': int(filtered_gold.sum()),\n",
    "        'matrix_shape': filtered_gold.shape,\n",
    "    }\n",
    "    return results, context\n",
    "\n",
    "\n",
    "def flatten_metrics(nested_metrics):\n",
    "    flat = {}\n",
    "    for block, metrics in nested_metrics.items():\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (list, tuple, dict)):\n",
    "                continue\n",
    "            arr = np.asarray(value)\n",
    "            if arr.ndim == 0:\n",
    "                label = key if block != 'overall' else f'{block}::{key}'\n",
    "                flat[label] = float(arr)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26310c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_RESULTS = {}\n",
    "EVALUATION_CONTEXT = {}\n",
    "EVALUATION_SUMMARY = []\n",
    "EVALUATION_ERRORS = []\n",
    "\n",
    "for alias in RUNS_TO_COMPARE:\n",
    "    cfg = GROUND_TRUTH_CONFIG.get(alias)\n",
    "    if cfg is None:\n",
    "        print(f'Skipping {alias}: no ground truth configuration available.')\n",
    "        continue\n",
    "    try:\n",
    "        metrics, context = evaluate_alias(alias, predictions_df, cfg)\n",
    "    except Exception as exc:\n",
    "        print(f'⚠️ {alias}: {exc}')\n",
    "        EVALUATION_ERRORS.append({'alias': alias, 'error': str(exc)})\n",
    "        continue\n",
    "    EVALUATION_RESULTS[alias] = metrics\n",
    "    EVALUATION_CONTEXT[alias] = context\n",
    "    flattened = flatten_metrics(metrics)\n",
    "    row = {'alias': alias, **flattened}\n",
    "    EVALUATION_SUMMARY.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(EVALUATION_SUMMARY).set_index('alias') if EVALUATION_SUMMARY else pd.DataFrame()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0976d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = pd.DataFrame.from_dict(EVALUATION_CONTEXT, orient='index') if EVALUATION_CONTEXT else pd.DataFrame()\n",
    "context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa469df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(EVALUATION_ERRORS) if EVALUATION_ERRORS else pd.DataFrame(columns=['alias', 'error'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
