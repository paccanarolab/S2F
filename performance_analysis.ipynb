{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98df36bf",
   "metadata": {},
   "source": [
    "# S2F Performance Analysis\n",
    "\n",
    "Use this notebook to explore the prediction outputs produced by different S2F runs and compare their behaviour. Each run writes a `prediction.df` file inside `<installation_directory>/output/<alias>/`. Update the configuration cells below with the aliases you want to analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4652",
   "metadata": {},
   "source": [
    "## Imports and configuration\n",
    "\n",
    "The snippet below reads `s2f.conf` to locate the shared output directory. Adjust `CONFIG_PATH` if you are running the notebook from a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a30beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import configparser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "CONFIG_PATH = Path('s2f.conf')\n",
    "config = configparser.ConfigParser()\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f'Configuration file not found: {CONFIG_PATH.resolve()}. Update CONFIG_PATH to match your setup.')\n",
    "config.read(CONFIG_PATH)\n",
    "\n",
    "BASE_OUTPUT = Path(config['directories']['installation_directory']).expanduser() / 'output'\n",
    "if not BASE_OUTPUT.exists():\n",
    "    raise FileNotFoundError(f'Output directory not found: {BASE_OUTPUT}. Make sure the S2F runs have been executed.')\n",
    "\n",
    "BASE_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde871d",
   "metadata": {},
   "source": [
    "## Discover available runs\n",
    "\n",
    "This cell lists the aliases that currently have diffusion outputs. If the list is long, slice or filter it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_RUNS = sorted(p.name for p in BASE_OUTPUT.iterdir() if p.is_dir())\n",
    "print(f\"{len(AVAILABLE_RUNS)} run(s) found under {BASE_OUTPUT}\")\n",
    "pd.DataFrame({'alias': AVAILABLE_RUNS})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5f3b6",
   "metadata": {},
   "source": [
    "## Select runs to compare\n",
    "\n",
    "Edit `RUNS_TO_COMPARE` to focus on the runs you are interested in. By default the cell keeps only aliases that actually exist in `AVAILABLE_RUNS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS_TO_COMPARE = [\n",
    "    'test_223283',\n",
    "    'test_223283_new',\n",
    "    'test_223283_new_2',\n",
    "]\n",
    "\n",
    "RUNS_TO_COMPARE = [alias for alias in RUNS_TO_COMPARE if alias in AVAILABLE_RUNS]\n",
    "if not RUNS_TO_COMPARE:\n",
    "    raise ValueError('Update RUNS_TO_COMPARE with at least one valid alias.')\n",
    "\n",
    "RUNS_TO_COMPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3f19",
   "metadata": {},
   "source": [
    "## Load prediction tables\n",
    "\n",
    "Helper functions to read the prediction scores and the index files written by each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253edffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Load the diffusion output for a single run as a tidy DataFrame.\"\"\"\n",
    "    path = base_path / alias / 'prediction.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Prediction file not found for {alias}: {path}')\n",
    "    df = pd.read_csv(path, sep='\t', header=None, names=['protein_id', 'term_id', 'score'])\n",
    "    df['alias'] = alias\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_terms(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Grab the GO term lookup table if you need term names or namespaces.\"\"\"\n",
    "    path = base_path / alias / 'terms.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'GO term index not found for {alias}: {path}')\n",
    "    return pd.read_pickle(path)\n",
    "\n",
    "\n",
    "def load_proteins(alias: str, base_path: Path = BASE_OUTPUT) -> pd.DataFrame:\n",
    "    \"\"\"Fetch the protein index for convenience (e.g. to map back to FASTA identifiers).\"\"\"\n",
    "    path = base_path / alias / 'proteins.df'\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Protein index not found for {alias}: {path}')\n",
    "    return pd.read_pickle(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.concat(\n",
    "    [load_predictions(alias) for alias in RUNS_TO_COMPARE],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {predictions_df.shape[0]:,} scored annotations spanning {predictions_df['alias'].nunique()} run(s).\")\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036f5f2",
   "metadata": {},
   "source": [
    "## Summary statistics per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    predictions_df\n",
    "    .groupby('alias')\n",
    "    .agg(\n",
    "        proteins=('protein_id', 'nunique'),\n",
    "        go_terms=('term_id', 'nunique'),\n",
    "        annotations=('term_id', 'size'),\n",
    "        min_score=('score', 'min'),\n",
    "        median_score=('score', 'median'),\n",
    "        mean_score=('score', 'mean'),\n",
    "        max_score=('score', 'max')\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39bd17",
   "metadata": {},
   "source": [
    "## Score distribution (log scale)\n",
    "\n",
    "Scores tend to be very small, so plotting their log<sub>10</sub> values highlights differences between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['score_log10'] = np.log10(predictions_df['score'].clip(lower=1e-15))\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "sns.histplot(\n",
    "    data=predictions_df,\n",
    "    x='score_log10',\n",
    "    hue='alias',\n",
    "    bins=60,\n",
    "    element='step',\n",
    "    stat='density',\n",
    "    common_norm=False,\n",
    "    alpha=0.35\n",
    ")\n",
    "plt.xlabel('log10(score)')\n",
    "plt.ylabel('density')\n",
    "plt.title('Score distribution per run (log scale)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595328f",
   "metadata": {},
   "source": [
    "## Focus on top-N predictions per protein\n",
    "\n",
    "Restrict to the highest-scoring annotations per protein to study agreement between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac49a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 5  # change to inspect more or fewer annotations per protein\n",
    "\n",
    "_top_sorted = predictions_df.sort_values(\n",
    "    ['alias', 'protein_id', 'score'],\n",
    "    ascending=[True, True, False]\n",
    ")\n",
    "\n",
    "top_predictions = (\n",
    "    _top_sorted\n",
    "    .groupby(['alias', 'protein_id'], as_index=False)\n",
    "    .head(TOP_N)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "top_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138b928",
   "metadata": {},
   "source": [
    "## Pairwise comparison (first two runs)\n",
    "\n",
    "The cell below contrasts the first two aliases in `RUNS_TO_COMPARE`. Edit `BASELINE` and `VARIANT` if you prefer a different pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2:\n",
    "    BASELINE, VARIANT = RUNS_TO_COMPARE[0], RUNS_TO_COMPARE[1]\n",
    "\n",
    "    baseline_top = (\n",
    "        top_predictions[top_predictions['alias'] == BASELINE]\n",
    "        [['protein_id', 'term_id', 'score']]\n",
    "        .set_index(['protein_id', 'term_id'])\n",
    "    )\n",
    "    variant_top = (\n",
    "        top_predictions[top_predictions['alias'] == VARIANT]\n",
    "        [['protein_id', 'term_id', 'score']]\n",
    "        .set_index(['protein_id', 'term_id'])\n",
    "    )\n",
    "\n",
    "    pairwise = baseline_top.join(\n",
    "        variant_top,\n",
    "        how='outer',\n",
    "        lsuffix=f'_{BASELINE}',\n",
    "        rsuffix=f'_{VARIANT}'\n",
    "    )\n",
    "    pairwise[f'score_{BASELINE}'] = pairwise.get(f'score_{BASELINE}', 0).fillna(0)\n",
    "    pairwise[f'score_{VARIANT}'] = pairwise.get(f'score_{VARIANT}', 0).fillna(0)\n",
    "    pairwise['score_delta'] = pairwise[f'score_{VARIANT}'] - pairwise[f'score_{BASELINE}']\n",
    "    pairwise = pairwise.sort_values('score_delta', ascending=False)\n",
    "\n",
    "    print(f'Comparing top-{TOP_N} predictions between {BASELINE} and {VARIANT}.')\n",
    "    pairwise.head(20)\n",
    "else:\n",
    "    print('Add at least two aliases to RUNS_TO_COMPARE to compute pairwise deltas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa83f2",
   "metadata": {},
   "source": [
    "## Largest improvements and regressions\n",
    "\n",
    "Filter the `pairwise` table to highlight the strongest positive or negative score shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c52ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2 and 'pairwise' in globals():\n",
    "    gains = pairwise[pairwise['score_delta'] > 0].head(20)\n",
    "    losses = pairwise[pairwise['score_delta'] < 0].tail(20)\n",
    "\n",
    "    print('Top gains:')\n",
    "    display(gains)\n",
    "\n",
    "    print('Top losses:')\n",
    "    display(losses)\n",
    "else:\n",
    "    print('Pairwise deltas are not available â€” add at least two aliases and run the previous cell first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e49f9",
   "metadata": {},
   "source": [
    "## Per-protein overlap statistics\n",
    "\n",
    "Compute how much the top-N annotations overlap per protein between the first two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46094b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(RUNS_TO_COMPARE) >= 2:\n",
    "    BASELINE, VARIANT = RUNS_TO_COMPARE[0], RUNS_TO_COMPARE[1]\n",
    "    overlaps = []\n",
    "\n",
    "    for protein_id, subset in top_predictions.groupby('protein_id'):\n",
    "        alias_groups = {alias: group for alias, group in subset.groupby('alias')}\n",
    "        if BASELINE not in alias_groups or VARIANT not in alias_groups:\n",
    "            continue\n",
    "        baseline_terms = set(alias_groups[BASELINE]['term_id'])\n",
    "        variant_terms = set(alias_groups[VARIANT]['term_id'])\n",
    "        union = baseline_terms | variant_terms\n",
    "        if not union:\n",
    "            continue\n",
    "        shared = baseline_terms & variant_terms\n",
    "        overlaps.append({\n",
    "            'protein_id': protein_id,\n",
    "            'shared': len(shared),\n",
    "            'union': len(union),\n",
    "            'jaccard': len(shared) / len(union)\n",
    "        })\n",
    "\n",
    "    overlap_df = pd.DataFrame(overlaps)\n",
    "    print(f'Computed overlaps for {len(overlap_df)} proteins present in both runs.')\n",
    "    overlap_df.sort_values('jaccard', ascending=False).head(20)\n",
    "else:\n",
    "    print('Add at least two aliases to RUNS_TO_COMPARE to evaluate overlap.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f564c",
   "metadata": {},
   "source": [
    "## Optional: attach GO term metadata\n",
    "\n",
    "If you need GO term names or namespaces, use `load_terms` and merge on `term_id`. The snippet below shows how to expand the pairwise comparison with GO labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf042cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: enrich pairwise table with GO term metadata (adjust `ALIAS_FOR_METADATA` as needed)\n",
    "if RUNS_TO_COMPARE:\n",
    "    ALIAS_FOR_METADATA = RUNS_TO_COMPARE[0]\n",
    "    try:\n",
    "        terms_lookup = load_terms(ALIAS_FOR_METADATA)\n",
    "        terms_lookup = terms_lookup.reset_index().rename(columns={'term id': 'term_id'})\n",
    "    except FileNotFoundError as exc:\n",
    "        print(exc)\n",
    "    else:\n",
    "        if 'pairwise' in globals():\n",
    "            decorated = pairwise.reset_index().merge(terms_lookup, on='term_id', how='left')\n",
    "            decorated.head()\n",
    "        else:\n",
    "            print('Run the pairwise comparison cell first to create the `pairwise` table.')\n",
    "else:\n",
    "    print('RUNS_TO_COMPARE is empty; nothing to decorate.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
